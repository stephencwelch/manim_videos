{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07bc727-4006-4d6e-b61a-7083d254c991",
   "metadata": {},
   "source": [
    "## Training 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5307f-4681-46d6-9ba9-2b2afbdca4e4",
   "metadata": {},
   "source": [
    "- Was hoping to not have to train myself, but not super surprised that I need to. ]\n",
    "- Starting here: https://colab.research.google.com/drive/1F6_1_cWXE5M7WocUcpQWp3v8z4b1jL20#scrollTo=Jpfq01sZyPdC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13a8868-a2b3-4a81-a9d3-026fbafb7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448eaccf-524a-43d7-8369-ad795272487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# from google.colab import drive\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# import comet_ml\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a113abb3-de4c-4da2-bbf1-62574012a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
    "# It's a dummy module that is the identity function by default\n",
    "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
    "# way to add PyTorch hooks\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "    \n",
    "    def give_name(self, name):\n",
    "        # Called by the model at initialisation\n",
    "        self.name = name\n",
    "    \n",
    "    def add_hook(self, hook, dir='fwd'):\n",
    "        # Hook format is fn(activation, hook_name)\n",
    "        # Change it into PyTorch hook format (this includes input and output, \n",
    "        # which are the same for a HookPoint)\n",
    "        def full_hook(module, module_input, module_output):\n",
    "            return hook(module_output, name=self.name)\n",
    "        if dir=='fwd':\n",
    "            handle = self.register_forward_hook(full_hook)\n",
    "            self.fwd_hooks.append(handle)\n",
    "        elif dir=='bwd':\n",
    "            handle = self.register_backward_hook(full_hook)\n",
    "            self.bwd_hooks.append(handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def remove_hooks(self, dir='fwd'):\n",
    "        if (dir=='fwd') or (dir=='both'):\n",
    "            for hook in self.fwd_hooks:\n",
    "                hook.remove()\n",
    "            self.fwd_hooks = []\n",
    "        if (dir=='bwd') or (dir=='both'):\n",
    "            for hook in self.bwd_hooks:\n",
    "                hook.remove()\n",
    "            self.bwd_hooks = []\n",
    "        if dir not in ['fwd', 'bwd', 'both']:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfd63e6-c764-4209-9408-cb466b1dcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "# I defined my own transformer from scratch so I'd fully understand each component \n",
    "# - I expect this wasn't necessary or particularly important, and a bunch of this \n",
    "# replicates existing PyTorch functionality\n",
    "\n",
    "# Embed & Unembed\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "# Positional Embeddings\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "# LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        self.hook_k = HookPoint()\n",
    "        self.hook_q = HookPoint()\n",
    "        self.hook_v = HookPoint()\n",
    "        self.hook_z = HookPoint()\n",
    "        self.hook_attn = HookPoint()\n",
    "        self.hook_attn_pre = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.hook_k(torch.einsum('ihd,bpd->biph', self.W_K, x))\n",
    "        q = self.hook_q(torch.einsum('ihd,bpd->biph', self.W_Q, x))\n",
    "        v = self.hook_v(torch.einsum('ihd,bpd->biph', self.W_V, x))\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n",
    "        z = self.hook_z(torch.einsum('biph,biqp->biqh', v, attn_matrix))\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "# MLP Layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        self.hook_pre = HookPoint()\n",
    "        self.hook_post = HookPoint()\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hook_pre(torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        x = self.hook_post(x)\n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "        self.hook_attn_out = HookPoint()\n",
    "        self.hook_mlp_out = HookPoint()\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n",
    "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
    "        return x\n",
    "\n",
    "# Full transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_cache=False, use_ln=True):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module)==HookPoint:\n",
    "                module.give_name(name)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "    \n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if 'hook' in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks('fwd')\n",
    "            hp.remove_hooks('bwd')\n",
    "    \n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name+'_grad'] = tensor[0].detach()\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, 'fwd')\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, 'bwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ba2e2c-f70a-430a-b579-7e5fe447aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def cuda_memory():\n",
    "    print(torch.cuda.memory_allocated()/1e9)\n",
    "\n",
    "def cross_entropy_high_precision(logits, labels):\n",
    "    # Shapes: batch x vocab, batch\n",
    "    # Cast logits to float64 because log_softmax has a float32 underflow on overly \n",
    "    # confident data and can only return multiples of 1.2e-7 (the smallest float x\n",
    "    # such that 1+x is different from 1 in float32). This leads to loss spikes \n",
    "    # and dodgy gradients\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels[:, None], dim=-1)\n",
    "    loss = -torch.mean(prediction_logprobs)\n",
    "    return loss\n",
    "\n",
    "def full_loss(model, data):\n",
    "    # Take the final position only\n",
    "    logits = model(data)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in data]).to('cuda')\n",
    "    return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def test_logits(logits, bias_correction=False, original_logits=None, mode='all'):\n",
    "    # Calculates cross entropy loss of logits representing a batch of all p^2 \n",
    "    # possible inputs\n",
    "    # Batch dimension is assumed to be first\n",
    "    if logits.shape[1]==p*p:\n",
    "        logits = logits.T\n",
    "    if logits.shape==torch.Size([p*p, p+1]):\n",
    "        logits = logits[:, :-1]\n",
    "    logits = logits.reshape(p*p, p)\n",
    "    if bias_correction:\n",
    "        # Applies bias correction - we correct for any missing bias terms, \n",
    "        # independent of the input, by centering the new logits along the batch \n",
    "        # dimension, and then adding the average original logits across all inputs\n",
    "        logits = einops.reduce(original_logits - logits, 'batch ... -> ...', 'mean') + logits\n",
    "    if mode=='train':\n",
    "        return cross_entropy_high_precision(logits[is_train], labels[is_train])\n",
    "    elif mode=='test':\n",
    "        return cross_entropy_high_precision(logits[is_test], labels[is_test])\n",
    "    elif mode=='all':\n",
    "        return cross_entropy_high_precision(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a382bac-439d-47da-b1b0-eee17c652686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "# This is mostly a bunch of over-engineered mess to hack Plotly into producing \n",
    "# the pretty pictures I want, I recommend not reading too closely unless you \n",
    "# want Plotly hacking practice\n",
    "def to_numpy(tensor, flat=False):\n",
    "    if type(tensor)!=torch.Tensor:\n",
    "        return tensor\n",
    "    if flat:\n",
    "        return tensor.flatten().detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "def imshow(tensor, xaxis=None, yaxis=None, animation_name='Snapshot', **kwargs):\n",
    "    if tensor.shape[0]==p*p:\n",
    "        tensor = unflatten_first(tensor)\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    px.imshow(to_numpy(tensor, flat=False), \n",
    "              labels={'x':xaxis, 'y':yaxis, 'animation_name':animation_name}, \n",
    "              **kwargs).show()\n",
    "# Set default colour scheme\n",
    "imshow = partial(imshow, color_continuous_scale='Blues')\n",
    "# Creates good defaults for showing divergent colour scales (ie with both \n",
    "# positive and negative values, where 0 is white)\n",
    "imshow_div = partial(imshow, color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "# Presets a bunch of defaults to imshow to make it suitable for showing heatmaps \n",
    "# of activations with x axis being input 1 and y axis being input 2.\n",
    "inputs_heatmap = partial(imshow, xaxis='Input 1', yaxis='Input 2', color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "def line(x, y=None, hover=None, xaxis='', yaxis='', **kwargs):\n",
    "    if type(y)==torch.Tensor:\n",
    "        y = to_numpy(y, flat=True)\n",
    "    if type(x)==torch.Tensor:\n",
    "        x=to_numpy(x, flat=True)\n",
    "    fig = px.line(x, y=y, hover_name=hover, **kwargs)\n",
    "    fig.update_layout(xaxis_title=xaxis, yaxis_title=yaxis)\n",
    "    fig.show()\n",
    "def scatter(x, y, **kwargs):\n",
    "    px.scatter(x=to_numpy(x, flat=True), y=to_numpy(y, flat=True), **kwargs).show()\n",
    "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
    "    # Helper function to plot multiple lines\n",
    "    if type(lines_list)==torch.Tensor:\n",
    "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
    "    if x is None:\n",
    "        x=np.arange(len(lines_list[0]))\n",
    "    fig = go.Figure(layout={'title':title})\n",
    "    fig.update_xaxes(title=xaxis)\n",
    "    fig.update_yaxes(title=yaxis)\n",
    "    for c, line in enumerate(lines_list):\n",
    "        if type(line)==torch.Tensor:\n",
    "            line = to_numpy(line)\n",
    "        if labels is not None:\n",
    "            label = labels[c]\n",
    "        else:\n",
    "            label = c\n",
    "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
    "    if log_y:\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()\n",
    "def line_marker(x, **kwargs):\n",
    "    lines([x], mode='lines+markers', **kwargs)\n",
    "def animate_lines(lines_list, snapshot_index = None, snapshot='snapshot', hover=None, xaxis='x', yaxis='y', **kwargs):\n",
    "    if type(lines_list)==list:\n",
    "        lines_list = torch.stack(lines_list, axis=0)\n",
    "    lines_list = to_numpy(lines_list, flat=False)\n",
    "    if snapshot_index is None:\n",
    "        snapshot_index = np.arange(lines_list.shape[0])\n",
    "    if hover is not None:\n",
    "        hover = [i for j in range(len(snapshot_index)) for i in hover]\n",
    "    print(lines_list.shape)\n",
    "    rows=[]\n",
    "    for i in range(lines_list.shape[0]):\n",
    "        for j in range(lines_list.shape[1]):\n",
    "            rows.append([lines_list[i][j], snapshot_index[i], j])\n",
    "    df = pd.DataFrame(rows, columns=[yaxis, snapshot, xaxis])\n",
    "    px.line(df, x=xaxis, y=yaxis, animation_frame=snapshot, range_y=[lines_list.min(), lines_list.max()], hover_name=hover,**kwargs).show()\n",
    "\n",
    "def imshow_fourier(tensor, title='', animation_name='snapshot', facet_labels=[], **kwargs):\n",
    "    # Set nice defaults for plotting functions in the 2D fourier basis\n",
    "    # tensor is assumed to already be in the Fourier Basis\n",
    "    if tensor.shape[0]==p*p:\n",
    "        tensor = unflatten_first(tensor)\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    fig=px.imshow(to_numpy(tensor),\n",
    "            x=fourier_basis_names, \n",
    "            y=fourier_basis_names, \n",
    "            labels={'x':'x Component', \n",
    "                    'y':'y Component', \n",
    "                    'animation_frame':animation_name},\n",
    "            title=title,\n",
    "            color_continuous_midpoint=0., \n",
    "            color_continuous_scale='RdBu', \n",
    "            **kwargs)\n",
    "    fig.update(data=[{'hovertemplate':\"%{x}x * %{y}y<br>Value:%{z:.4f}\"}])\n",
    "    if facet_labels:\n",
    "        for i, label in enumerate(facet_labels):\n",
    "            fig.layout.annotations[i]['text'] = label\n",
    "    fig.show()\n",
    "\n",
    "def animate_multi_lines(lines_list, y_index=None, snapshot_index = None, snapshot='snapshot', hover=None, swap_y_animate=False, **kwargs):\n",
    "    # Can plot an animation of lines with multiple lines on the plot.\n",
    "    if type(lines_list)==list:\n",
    "        lines_list = torch.stack(lines_list, axis=0)\n",
    "    lines_list = to_numpy(lines_list, flat=False)\n",
    "    if swap_y_animate:\n",
    "        lines_list = lines_list.transpose(1, 0, 2)\n",
    "    if snapshot_index is None:\n",
    "        snapshot_index = np.arange(lines_list.shape[0])\n",
    "    if y_index is None:\n",
    "        y_index = [str(i) for i in range(lines_list.shape[1])]\n",
    "    if hover is not None:\n",
    "        hover = [i for j in range(len(snapshot_index)) for i in hover]\n",
    "    print(lines_list.shape)\n",
    "    rows=[]\n",
    "    for i in range(lines_list.shape[0]):\n",
    "        for j in range(lines_list.shape[2]):\n",
    "            rows.append(list(lines_list[i, :, j])+[snapshot_index[i], j])\n",
    "    df = pd.DataFrame(rows, columns=y_index+[snapshot, 'x'])\n",
    "    px.line(df, x='x', y=y_index, animation_frame=snapshot, range_y=[lines_list.min(), lines_list.max()], hover_name=hover, **kwargs).show()\n",
    "\n",
    "def animate_scatter(lines_list, snapshot_index = None, snapshot='snapshot', hover=None, yaxis='y', xaxis='x', color=None, color_name = 'color', **kwargs):\n",
    "    # Can plot an animated scatter plot\n",
    "    # lines_list has shape snapshot x 2 x line\n",
    "    if type(lines_list)==list:\n",
    "        lines_list = torch.stack(lines_list, axis=0)\n",
    "    lines_list = to_numpy(lines_list, flat=False)\n",
    "    if snapshot_index is None:\n",
    "        snapshot_index = np.arange(lines_list.shape[0])\n",
    "    if hover is not None:\n",
    "        hover = [i for j in range(len(snapshot_index)) for i in hover]\n",
    "    if color is None:\n",
    "        color = np.ones(lines_list.shape[-1])\n",
    "    if type(color)==torch.Tensor:\n",
    "        color = to_numpy(color)\n",
    "    if len(color.shape)==1:\n",
    "        color = einops.repeat(color, 'x -> snapshot x', snapshot=lines_list.shape[0])\n",
    "    print(lines_list.shape)\n",
    "    rows=[]\n",
    "    for i in range(lines_list.shape[0]):\n",
    "        for j in range(lines_list.shape[2]):\n",
    "            rows.append([lines_list[i, 0, j].item(), lines_list[i, 1, j].item(), snapshot_index[i], color[i, j]])\n",
    "    print([lines_list[:, 0].min(), lines_list[:, 0].max()])\n",
    "    print([lines_list[:, 1].min(), lines_list[:, 1].max()])\n",
    "    df = pd.DataFrame(rows, columns=[xaxis, yaxis, snapshot, color_name])\n",
    "    px.scatter(df, x=xaxis, y=yaxis, animation_frame=snapshot, range_x=[lines_list[:, 0].min(), lines_list[:, 0].max()], range_y=[lines_list[:, 1].min(), lines_list[:, 1].max()], hover_name=hover, color=color_name, **kwargs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06b8194-56cc-4d2d-9800-239e80d586f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_first(tensor):\n",
    "    if tensor.shape[0]==p*p:\n",
    "        return einops.rearrange(tensor, '(x y) ... -> x y ...', x=p, y=p)\n",
    "    else: \n",
    "        return tensor\n",
    "def cos(x, y):\n",
    "    return (x.dot(y))/x.norm()/y.norm()\n",
    "def mod_div(a, b):\n",
    "    return (a*pow(b, p-2, p))%p\n",
    "def normalize(tensor, axis=0):\n",
    "    return tensor/(tensor).pow(2).sum(keepdim=True, axis=axis).sqrt()\n",
    "def extract_freq_2d(tensor, freq):\n",
    "    # Takes in a pxpx... or batch x ... tensor, returns a 3x3x... tensor of the \n",
    "    # Linear and quadratic terms of frequency freq\n",
    "    tensor = unflatten_first(tensor)\n",
    "    # Extracts the linear and quadratic terms corresponding to frequency freq\n",
    "    index_1d = [0, 2*freq-1, 2*freq]\n",
    "    # Some dumb manipulation to use fancy array indexing rules\n",
    "    # Gets the rows and columns in index_1d\n",
    "    return tensor[[[i]*3 for i in index_1d], [index_1d]*3]\n",
    "def get_cov(tensor, norm=True):\n",
    "    # Calculate covariance matrix\n",
    "    if norm:\n",
    "        tensor = normalize(tensor, axis=1)\n",
    "    return tensor @ tensor.T\n",
    "def is_close(a, b):\n",
    "    return ((a-b).pow(2).sum()/(a.pow(2).sum().sqrt())/(b.pow(2).sum().sqrt())).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13280c20-1f19-451a-a82b-7b15e67f0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_components(m):\n",
    "    \"\"\"\n",
    "    Compute canonical frequency components of a real 2D signal m using fft2.\n",
    "\n",
    "    Returns:\n",
    "        components: list of dicts with keys:\n",
    "            - 'kx': frequency index in x (rows)\n",
    "            - 'ky': frequency index in y (cols)\n",
    "            - 'coeff': complex FFT coefficient at (kx, ky)\n",
    "        Nx, Ny: dimensions of the input\n",
    "    \"\"\"\n",
    "    m = np.asarray(m)\n",
    "    Nx, Ny = m.shape\n",
    "    F = np.fft.fft2(m)\n",
    "\n",
    "    components = []\n",
    "    seen = set()\n",
    "\n",
    "    for kx in range(Nx):\n",
    "        for ky in range(Ny):\n",
    "            # Conjugate partner indices for real-valued input\n",
    "            kx_conj = (-kx) % Nx\n",
    "            ky_conj = (-ky) % Ny\n",
    "\n",
    "            pair = tuple(sorted([(kx, ky), (kx_conj, ky_conj)]))\n",
    "            if pair in seen:\n",
    "                continue\n",
    "            seen.add(pair)\n",
    "\n",
    "            # Always store the \"smaller\" one in lexicographic order\n",
    "            kx_rep, ky_rep = pair[0]\n",
    "            coeff = F[kx_rep, ky_rep]\n",
    "            components.append({\n",
    "                \"kx\": kx_rep,\n",
    "                \"ky\": ky_rep,\n",
    "                \"coeff\": coeff\n",
    "            })\n",
    "\n",
    "    return components, Nx, Ny\n",
    "\n",
    "\n",
    "def components_to_terms(components, Nx, Ny, num_freqs, include_dc=True):\n",
    "    \"\"\"\n",
    "    Convert FFT components into real cosine terms with amplitude & phase.\n",
    "\n",
    "    Args:\n",
    "        components: output from compute_frequency_components\n",
    "        Nx, Ny: grid size\n",
    "        num_freqs: number of *non-DC* frequencies to keep\n",
    "        include_dc: whether to include the constant (DC) term\n",
    "\n",
    "    Returns:\n",
    "        terms: list of dicts with keys:\n",
    "            - 'kx', 'ky'\n",
    "            - 'amplitude'  (real, >= 0)\n",
    "            - 'phase'      (in radians)\n",
    "            - 'is_dc'      (bool, True only for the constant term)\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    dc_term = None\n",
    "\n",
    "    for c in components:\n",
    "        kx = c[\"kx\"]\n",
    "        ky = c[\"ky\"]\n",
    "        coeff = c[\"coeff\"]\n",
    "\n",
    "        # DC component (constant offset)\n",
    "        if kx == 0 and ky == 0:\n",
    "            dc_value = coeff.real / (Nx * Ny)\n",
    "            dc_term = {\n",
    "                \"kx\": 0,\n",
    "                \"ky\": 0,\n",
    "                \"amplitude\": dc_value,  # directly the constant offset\n",
    "                \"phase\": 0.0,\n",
    "                \"is_dc\": True,\n",
    "            }\n",
    "        else:\n",
    "            # For real signals, combine (k, l) and (-k, -l) into:\n",
    "            #   2*|F|/(Nx*Ny) * cos(2π(kx i/Nx + ky j/Ny) + phase)\n",
    "            amp = 2.0 * np.abs(coeff) / (Nx * Ny)\n",
    "            phase = np.angle(coeff)\n",
    "            terms.append({\n",
    "                \"kx\": kx,\n",
    "                \"ky\": ky,\n",
    "                \"amplitude\": amp,\n",
    "                \"phase\": phase,\n",
    "                \"is_dc\": False,\n",
    "            })\n",
    "\n",
    "    # Sort non-DC terms by amplitude (largest first)\n",
    "    terms.sort(key=lambda t: t[\"amplitude\"], reverse=True)\n",
    "\n",
    "    # Keep only the top num_freqs\n",
    "    terms = terms[:num_freqs]\n",
    "\n",
    "    # Optionally prepend DC term\n",
    "    if include_dc and dc_term is not None:\n",
    "        terms = [dc_term] + terms\n",
    "\n",
    "    return terms\n",
    "\n",
    "def generate_python_reconstructor(terms, Nx, Ny, func_name=\"recon_func\", decimals=4):\n",
    "    \"\"\"\n",
    "    Generate a Python function as a string that reconstructs the signal\n",
    "    using explicit 1D and product-of-cosine terms from a sparse Fourier representation.\n",
    "\n",
    "    Types of terms:\n",
    "      - DC term (constant)\n",
    "      - Row-only:   A * cos(2π * (kx*i/Nx) + φ)\n",
    "      - Col-only:   B * cos(2π * (ky*j/Ny) + φ)\n",
    "      - Product:    C * cos(2π * (kx*i/Nx) + φ) * cos(2π * (ky*j/Ny) + φ)\n",
    "\n",
    "    You must have `import numpy as np` in the scope where you paste/run this.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(i, j):\")\n",
    "\n",
    "    if not terms:\n",
    "        lines.append(\"    return 0.0\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Separate DC from others\n",
    "    dc_terms = [t for t in terms if t.get(\"is_dc\", False)]\n",
    "    non_dc_terms = [t for t in terms if not t.get(\"is_dc\", False)]\n",
    "\n",
    "    if dc_terms:\n",
    "        dc_val = round(dc_terms[0][\"amplitude\"], decimals)\n",
    "        lines.append(f\"    val = {dc_val}\")\n",
    "    else:\n",
    "        lines.append(\"    val = 0.0\")\n",
    "\n",
    "    for t in non_dc_terms:\n",
    "        kx, ky = t[\"kx\"], t[\"ky\"]\n",
    "        A = round(t[\"amplitude\"], decimals)\n",
    "        phi = round(t[\"phase\"], decimals)\n",
    "\n",
    "        if A == 0:\n",
    "            continue\n",
    "\n",
    "        if kx != 0 and ky == 0:\n",
    "            # Row-only cosine\n",
    "            line = (\n",
    "                f\"    val += {A} * np.cos(2*np.pi*(({kx}*i)/{Nx}) + {phi})\"\n",
    "            )\n",
    "        elif kx == 0 and ky != 0:\n",
    "            # Column-only cosine\n",
    "            line = (\n",
    "                f\"    val += {A} * np.cos(2*np.pi*(({ky}*j)/{Ny}) + {phi})\"\n",
    "            )\n",
    "        else:\n",
    "            # Product-of-cosines term (your desired \"third\" type)\n",
    "            line = (\n",
    "                f\"    val += {A} * np.cos(2*np.pi*(({kx}*i)/{Nx}) + {phi})\"\n",
    "                f\" * np.cos(2*np.pi*(({ky}*j)/{Ny}) + {phi})\"\n",
    "            )\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "    lines.append(\"    return val\")\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648b31bb-66b2-4ac4-ad40-1c565cff3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For all CUDA devices\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False # Set to False for determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d167e-41d5-4f42-aedc-94df6f39dcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c9596-9056-49dc-9125-f6d78378faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7686f161-f5b6-46b6-baaf-2761c3064ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_train=0.3 #0.28 might be more interesting\n",
    "p=113\n",
    "# seed=2 ##Set at top\n",
    "\n",
    "lr=1e-3 #@param\n",
    "weight_decay = 1.0 #@param\n",
    "p=113 #@param\n",
    "d_model = 128 #@param\n",
    "fn_name = 'add' #@param ['add', 'subtract', 'x2xyy2','rand']\n",
    "num_epochs = 12000 #@param\n",
    "save_models = False #@param\n",
    "save_every = 100 #@param\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1 #@param\n",
    "\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1\n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "# batch_size = 512\n",
    "use_ln = False\n",
    "random_answers = np.random.randint(low=0, high=p, size=(p, p))\n",
    "fns_dict = {'add': lambda x,y:(x+y)%p, 'subtract': lambda x,y:(x-y)%p, 'x2xyy2':lambda x,y:(x**2+x*y+y**2)%p, 'rand':lambda x,y:random_answers[x][y]}\n",
    "fn = fns_dict[fn_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b5953f-f4cb-459b-b2a4-55a8163135d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 522)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_train, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8335ca5-3a4a-4e8c-a543-91efc3b8a0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3830 8939\n"
     ]
    }
   ],
   "source": [
    "def gen_train_test(frac_train, num, seed=0):\n",
    "    # Generate train and test split\n",
    "    pairs = [(i, j, num) for i in range(num) for j in range(num)]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "    div = int(frac_train*len(pairs))\n",
    "    return pairs[:div], pairs[div:]\n",
    "\n",
    "train, test = gen_train_test(frac_train, p, seed)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb1c010-e637-4e6a-9acc-b5ec2720fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an array of Boolean indices according to whether each data point is in \n",
    "# train or test\n",
    "# Used to index into the big batch of all possible data\n",
    "is_train = []\n",
    "is_test = []\n",
    "for x in range(p):\n",
    "    for y in range(p):\n",
    "        if (x, y, 113) in train:\n",
    "            is_train.append(True)\n",
    "            is_test.append(False)\n",
    "        else:\n",
    "            is_train.append(False)\n",
    "            is_test.append(True)\n",
    "is_train = np.array(is_train)\n",
    "is_test = np.array(is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abeb273b-eabd-432b-ba2a-72f3e7ac04f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name grok_1764096306\n",
      "0_1.5627_1.5625\n",
      "100_0.8256_2.0475\n",
      "200_-3.8913_2.7883\n",
      "300_-4.9221_2.8172\n",
      "400_-6.0467_2.8741\n",
      "500_-7.1463_2.9330\n",
      "600_-8.2310_2.9909\n",
      "700_-9.3020_3.0463\n",
      "800_-10.3575_3.0988\n",
      "900_-11.3883_3.1483\n",
      "1000_-12.3721_3.1931\n",
      "1100_-13.2604_3.2316\n",
      "1200_-13.9845_3.2609\n",
      "1300_-14.4839_3.2781\n",
      "1400_-14.7521_3.2839\n",
      "1500_-14.8536_3.2818\n",
      "1600_-14.8808_3.2763\n",
      "1700_-14.8864_3.2699\n",
      "1800_-14.8901_3.2637\n",
      "1900_-14.8931_3.2577\n",
      "2000_-14.8969_3.2518\n",
      "2100_-14.9000_3.2459\n",
      "2200_-14.9046_3.2405\n",
      "2300_-14.9084_3.2351\n",
      "2400_-14.9127_3.2297\n",
      "2500_-14.9170_3.2245\n",
      "2600_-14.9203_3.2193\n",
      "2700_-14.9249_3.2141\n",
      "2800_-14.9287_3.2089\n",
      "2900_-14.9330_3.2037\n",
      "3000_-14.9352_3.1985\n",
      "3100_-14.9406_3.1930\n",
      "3200_-14.9432_3.1877\n",
      "3300_-14.9477_3.1823\n",
      "3400_-14.9518_3.1769\n",
      "3500_-14.9535_3.1715\n",
      "3600_-14.9575_3.1659\n",
      "3700_-14.9608_3.1606\n",
      "3800_-14.9654_3.1550\n",
      "3900_-14.9687_3.1495\n",
      "4000_-14.9705_3.1438\n",
      "4100_-14.9746_3.1382\n",
      "4200_-14.9766_3.1325\n",
      "4300_-14.9817_3.1268\n",
      "4400_-14.9844_3.1205\n",
      "4500_-14.9862_3.1138\n",
      "4600_-14.9909_3.1068\n",
      "4700_-14.9935_3.0997\n",
      "4800_-14.9977_3.0924\n",
      "4900_-15.0007_3.0849\n",
      "5000_-15.0044_3.0768\n",
      "5100_-15.0079_3.0684\n",
      "5200_-15.0111_3.0595\n",
      "5300_-15.0128_3.0501\n",
      "5400_-15.0159_3.0402\n",
      "5500_-15.0215_3.0297\n",
      "5600_-15.0246_3.0186\n",
      "5700_-15.0283_3.0068\n",
      "5800_-15.0321_2.9944\n",
      "5900_-15.0335_2.9814\n",
      "6000_-15.0380_2.9676\n",
      "6100_-15.0435_2.9528\n",
      "6200_-15.0466_2.9368\n",
      "6300_-15.0508_2.9196\n",
      "6400_-15.0555_2.9017\n",
      "6500_-15.0595_2.8826\n",
      "6600_-15.0649_2.8622\n",
      "6700_-15.0705_2.8404\n",
      "6800_-15.0759_2.8168\n",
      "6900_-15.0827_2.7904\n",
      "7000_-15.0887_2.7605\n",
      "7100_-15.0976_2.7271\n",
      "7200_-15.1055_2.6896\n",
      "7300_-15.1149_2.6482\n",
      "7400_-15.1251_2.6015\n",
      "7500_-15.1350_2.5477\n",
      "7600_-15.1474_2.4854\n",
      "7700_-15.1605_2.4153\n",
      "7800_-15.1739_2.3355\n",
      "7900_-15.1931_2.2427\n",
      "8000_-15.2126_2.1340\n",
      "8100_-15.2337_2.0049\n",
      "8200_-15.2589_1.8535\n",
      "8300_-15.2892_1.6722\n",
      "8400_-15.3208_1.4572\n",
      "8500_-15.3623_1.1907\n",
      "8600_-15.4135_0.8461\n",
      "8700_-15.4718_0.4004\n",
      "8800_-15.5346_-0.1661\n",
      "8900_-15.5977_-0.8510\n",
      "9000_-15.6527_-1.6001\n",
      "9100_-15.7075_-2.3780\n",
      "9200_-15.7561_-3.1779\n",
      "9300_-15.8022_-3.9813\n",
      "9400_-15.8481_-4.8116\n",
      "9500_-15.8928_-5.7857\n",
      "9600_-15.9386_-6.9331\n",
      "9700_-15.9862_-7.9892\n",
      "9800_-16.0302_-9.5596\n",
      "9900_-16.0653_-11.2414\n",
      "10000_-16.0918_-12.5325\n",
      "10100_-16.1119_-13.4361\n",
      "10200_-16.1254_-14.0572\n",
      "10300_-16.1359_-14.4497\n",
      "10400_-16.1436_-14.7215\n",
      "10500_-16.1497_-14.9009\n",
      "10600_-16.1547_-15.0221\n",
      "10700_-16.1588_-15.1051\n",
      "10800_-16.1623_-15.1605\n",
      "10900_-16.1653_-15.2004\n",
      "11000_-16.1680_-15.2297\n",
      "11100_-16.1702_-15.2557\n",
      "11200_-16.1722_-15.2765\n",
      "11300_-16.1741_-15.2922\n",
      "11400_-16.1757_-15.3025\n",
      "11500_-16.1773_-15.3123\n",
      "11600_-16.1787_-15.3200\n",
      "11700_-16.1800_-15.3270\n",
      "11800_-16.1811_-15.3340\n",
      "11900_-16.1822_-15.3392\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot/run_name/\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_models:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     os.mkdir(\u001b[43mroot\u001b[49m/run_name)\n\u001b[32m     41\u001b[39m save_dict = {\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model.state_dict(),\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: optimizer.state_dict(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: epoch,\n\u001b[32m     50\u001b[39m }\n\u001b[32m     51\u001b[39m torch.save(save_dict, root/run_name/\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfinal.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "model = Transformer(num_layers=num_layers, d_vocab=d_vocab, d_model=d_model, d_mlp=d_mlp, d_head=d_head, num_heads=num_heads, n_ctx=n_ctx, act_type=act_type, use_cache=False, use_ln=use_ln)\n",
    "model.to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "run_name = f\"grok_{int(time.time())}\"\n",
    "print(f'Run name {run_name}')\n",
    "if save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "    save_dict = {'model':model.state_dict(), 'train_data':train, 'test_data':test}\n",
    "    torch.save(save_dict, root/run_name/'init.pth')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = full_loss(model, train)\n",
    "    test_loss = full_loss(model, test)\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    if epoch%100 == 0: print(f\"{epoch}_{np.log(train_loss.item()):.4f}_{np.log(test_loss.item()):.4f}\")#_{train_acc.item():.4f}_{test_acc.item():.4f}\")\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    if test_loss.item() < stopping_thresh:\n",
    "        break\n",
    "    if (save_models) and (epoch%save_every == 0):\n",
    "        if test_loss.item() < stopping_thresh:\n",
    "            break\n",
    "        save_dict = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_dict, root/run_name/f\"{epoch}.pth\")\n",
    "        print(f\"Saved model to {root/run_name/f'{epoch}.pth'}\")\n",
    "if not save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "save_dict = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_loss': train_loss,\n",
    "    'test_loss': test_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'epoch': epoch,\n",
    "}\n",
    "torch.save(save_dict, root/run_name/f\"final.pth\")\n",
    "print(f\"Saved model to {root/run_name/f'final.pth'}\")\n",
    "lines([train_losses, test_losses], labels=['train', 'test'], log_y=True)\n",
    "\n",
    "# save_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30969c1e-0a90-4295-948b-253296cdefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be7e2b-c96e-4ffe-bfd5-6e26bdfb2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10(train_losses)); plt.plot(np.log10(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a863d-b794-4dbc-ad3a-48927c72cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_losses); plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e66361-7c51-4163-a7ab-2669b9a66b87",
   "metadata": {},
   "source": [
    "Ok, how to quickly find frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f24e5-8898-40d9-a7dd-09d17aeac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.tensor([(i, j, p) for i in range(p) for j in range(p)]).to('cuda')\n",
    "labels = torch.tensor([fn(i, j) for i, j, _ in all_data]).to('cuda')\n",
    "cache = {}\n",
    "model.remove_all_hooks()\n",
    "model.cache_all(cache)\n",
    "# Final position only\n",
    "original_logits = model(all_data)[:, -1]\n",
    "# Remove equals sign from output logits\n",
    "original_logits = original_logits[:, :-1]\n",
    "original_loss = cross_entropy_high_precision(original_logits, labels)\n",
    "print(f\"Original loss: {original_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69df57-cd04-4976-b0a8-d302ac86ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in cache.keys():\n",
    "    print(k, cache[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b555d-8260-4903-865f-7fa871e8ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok let's try pre -> the og. \n",
    "mr = einops.rearrange(cache['blocks.0.mlp.hook_pre'], \"(x y) ... -> x y ...\", x=p).detach().cpu().numpy() \n",
    "mr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa125b7-a9d8-4f4a-8ee6-89cf1dd88b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0, (12,12))\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        fig.add_subplot(12,12,12*i+j+1)\n",
    "        plt.scatter(mr[:, 0, 2, i], mr[:, 0, 2, j], c=range(113), s=1)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77980445-a1a4-4254-829f-ee0a42019681",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx=0\n",
    "F = np.fft.fft2(mr[:,:,2,neuron_idx])\n",
    "\n",
    "components, Nx, Ny = compute_frequency_components(mr[:,:,2,neuron_idx])\n",
    "terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "code_str = generate_python_reconstructor(terms, Nx, Ny,\n",
    "                                         func_name=\"approx_M\",\n",
    "                                         decimals=3)\n",
    "print(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae233b3-41f1-4d20-934f-48f407b1fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx=1\n",
    "F = np.fft.fft2(mr[:,:,2,neuron_idx])\n",
    "\n",
    "components, Nx, Ny = compute_frequency_components(mr[:,:,2,neuron_idx])\n",
    "terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "code_str = generate_python_reconstructor(terms, Nx, Ny,\n",
    "                                         func_name=\"approx_M\",\n",
    "                                         decimals=3)\n",
    "print(code_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ff82e-290b-4102-b55d-63c3bd05e363",
   "metadata": {},
   "source": [
    "- Woah 3 and 6?! Damn I should have cached indemediate results!\n",
    "- If they actually show up for cross terms of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f51df-c02c-45d9-a26a-fe914653c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx=2\n",
    "F = np.fft.fft2(mr[:,:,2,neuron_idx])\n",
    "\n",
    "components, Nx, Ny = compute_frequency_components(mr[:,:,2,neuron_idx])\n",
    "terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "code_str = generate_python_reconstructor(terms, Nx, Ny,\n",
    "                                         func_name=\"approx_M\",\n",
    "                                         decimals=3)\n",
    "print(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80d820-fa1a-44ba-bb8c-928b8ea02986",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx=5\n",
    "F = np.fft.fft2(mr[:,:,2,neuron_idx])\n",
    "\n",
    "components, Nx, Ny = compute_frequency_components(mr[:,:,2,neuron_idx])\n",
    "terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "code_str = generate_python_reconstructor(terms, Nx, Ny,\n",
    "                                         func_name=\"approx_M\",\n",
    "                                         decimals=3)\n",
    "print(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f9db9-4765-4a74-a35d-f03a242e5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_M(i, j):\n",
    "    val = 0.216\n",
    "    val += 0.073 * np.cos(2*np.pi*((20*i)/113) + 1.604) * np.cos(2*np.pi*((20*j)/113) + 1.604)\n",
    "    val += 0.044 * np.cos(2*np.pi*((51*i)/113) + 2.097) * np.cos(2*np.pi*((51*j)/113) + 2.097)\n",
    "    val += 0.037 * np.cos(2*np.pi*((20*i)/113) + 3.142) * np.cos(2*np.pi*((93*j)/113) + 3.142)\n",
    "    val += 0.036 * np.cos(2*np.pi*((40*i)/113) + -1.563)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceacda3-afeb-4bf4-b624-a7491c99ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_approx=np.zeros((p,p))\n",
    "for i in range(p):\n",
    "    for j in range(p):\n",
    "        m_approx[i,j]=approx_M(i,j)\n",
    "\n",
    "fig=plt.figure(0, (12, 6))\n",
    "fig.add_subplot(1,3,1); plt.imshow(mr[:,:,2,neuron_idx]); plt.axis('off')\n",
    "fig.add_subplot(1,3,2); plt.imshow(np.abs(F));  plt.axis('off')\n",
    "fig.add_subplot(1,3,3); plt.imshow(m_approx); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48becc92-2584-406b-88de-edd422f89687",
   "metadata": {},
   "source": [
    "20 Might be better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246420fe-9875-4357-b200-c0309950835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron_idx in range(32):\n",
    "    F = np.fft.fft2(mr[:,:,2,neuron_idx])\n",
    "    \n",
    "    components, Nx, Ny = compute_frequency_components(mr[:,:,2,neuron_idx])\n",
    "    terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "    code_str = generate_python_reconstructor(terms, Nx, Ny,\n",
    "                                             func_name=\"approx_M\",\n",
    "                                             decimals=3)\n",
    "    print(neuron_idx, code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b924ff3-d2d8-4006-90d9-ee54d8ad6d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abc5de-f8dc-4ebc-ae7f-4c46b10e5c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a2f36-bcb3-4005-809b-776075cae70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0108b5e-f0cc-4321-9d64-df5fc1670e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf880678-a78a-498d-846d-a6c83b3c78b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252b3a3-cccb-4487-8cb3-9e1db94c9268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9cc17-af93-4864-b195-fc7c30326fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c1979-03c3-4c21-ab00-53e9a17a97de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
