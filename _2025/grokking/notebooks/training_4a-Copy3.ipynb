{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07bc727-4006-4d6e-b61a-7083d254c991",
   "metadata": {},
   "source": [
    "## Training 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a8868-a2b3-4a81-a9d3-026fbafb7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2\n",
    "frac_train=0.25 \n",
    "weight_decay = 1.0 #@param\n",
    "freq_penalty_weight = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448eaccf-524a-43d7-8369-ad795272487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# from google.colab import drive\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# import comet_ml\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98d724-54bb-4b49-8c4e-b7a10a2e23e3",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113abb3-de4c-4da2-bbf1-62574012a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
    "# It's a dummy module that is the identity function by default\n",
    "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
    "# way to add PyTorch hooks\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "    \n",
    "    def give_name(self, name):\n",
    "        # Called by the model at initialisation\n",
    "        self.name = name\n",
    "    \n",
    "    def add_hook(self, hook, dir='fwd'):\n",
    "        # Hook format is fn(activation, hook_name)\n",
    "        # Change it into PyTorch hook format (this includes input and output, \n",
    "        # which are the same for a HookPoint)\n",
    "        def full_hook(module, module_input, module_output):\n",
    "            return hook(module_output, name=self.name)\n",
    "        if dir=='fwd':\n",
    "            handle = self.register_forward_hook(full_hook)\n",
    "            self.fwd_hooks.append(handle)\n",
    "        elif dir=='bwd':\n",
    "            handle = self.register_backward_hook(full_hook)\n",
    "            self.bwd_hooks.append(handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def remove_hooks(self, dir='fwd'):\n",
    "        if (dir=='fwd') or (dir=='both'):\n",
    "            for hook in self.fwd_hooks:\n",
    "                hook.remove()\n",
    "            self.fwd_hooks = []\n",
    "        if (dir=='bwd') or (dir=='both'):\n",
    "            for hook in self.bwd_hooks:\n",
    "                hook.remove()\n",
    "            self.bwd_hooks = []\n",
    "        if dir not in ['fwd', 'bwd', 'both']:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd63e6-c764-4209-9408-cb466b1dcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "# I defined my own transformer from scratch so I'd fully understand each component \n",
    "# - I expect this wasn't necessary or particularly important, and a bunch of this \n",
    "# replicates existing PyTorch functionality\n",
    "\n",
    "# Embed & Unembed\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "# Positional Embeddings\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "# LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        self.hook_k = HookPoint()\n",
    "        self.hook_q = HookPoint()\n",
    "        self.hook_v = HookPoint()\n",
    "        self.hook_z = HookPoint()\n",
    "        self.hook_attn = HookPoint()\n",
    "        self.hook_attn_pre = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.hook_k(torch.einsum('ihd,bpd->biph', self.W_K, x))\n",
    "        q = self.hook_q(torch.einsum('ihd,bpd->biph', self.W_Q, x))\n",
    "        v = self.hook_v(torch.einsum('ihd,bpd->biph', self.W_V, x))\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n",
    "        z = self.hook_z(torch.einsum('biph,biqp->biqh', v, attn_matrix))\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "# MLP Layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        self.hook_pre = HookPoint()\n",
    "        self.hook_post = HookPoint()\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hook_pre(torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        x = self.hook_post(x)\n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "        self.hook_attn_out = HookPoint()\n",
    "        self.hook_mlp_out = HookPoint()\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n",
    "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
    "        return x\n",
    "\n",
    "# Full transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_cache=False, use_ln=True):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module)==HookPoint:\n",
    "                module.give_name(name)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "    \n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if 'hook' in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks('fwd')\n",
    "            hp.remove_hooks('bwd')\n",
    "    \n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name+'_grad'] = tensor[0].detach()\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, 'fwd')\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, 'bwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba2e2c-f70a-430a-b579-7e5fe447aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def cuda_memory():\n",
    "    print(torch.cuda.memory_allocated()/1e9)\n",
    "\n",
    "def cross_entropy_high_precision(logits, labels):\n",
    "    # Shapes: batch x vocab, batch\n",
    "    # Cast logits to float64 because log_softmax has a float32 underflow on overly \n",
    "    # confident data and can only return multiples of 1.2e-7 (the smallest float x\n",
    "    # such that 1+x is different from 1 in float32). This leads to loss spikes \n",
    "    # and dodgy gradients\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels[:, None], dim=-1)\n",
    "    loss = -torch.mean(prediction_logprobs)\n",
    "    return loss\n",
    "\n",
    "def full_loss(model, data):\n",
    "    # Take the final position only\n",
    "    logits = model(data)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in data]).to('cuda')\n",
    "    return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def test_logits(logits, bias_correction=False, original_logits=None, mode='all'):\n",
    "    # Calculates cross entropy loss of logits representing a batch of all p^2 \n",
    "    # possible inputs\n",
    "    # Batch dimension is assumed to be first\n",
    "    if logits.shape[1]==p*p:\n",
    "        logits = logits.T\n",
    "    if logits.shape==torch.Size([p*p, p+1]):\n",
    "        logits = logits[:, :-1]\n",
    "    logits = logits.reshape(p*p, p)\n",
    "    if bias_correction:\n",
    "        # Applies bias correction - we correct for any missing bias terms, \n",
    "        # independent of the input, by centering the new logits along the batch \n",
    "        # dimension, and then adding the average original logits across all inputs\n",
    "        logits = einops.reduce(original_logits - logits, 'batch ... -> ...', 'mean') + logits\n",
    "    if mode=='train':\n",
    "        return cross_entropy_high_precision(logits[is_train], labels[is_train])\n",
    "    elif mode=='test':\n",
    "        return cross_entropy_high_precision(logits[is_test], labels[is_test])\n",
    "    elif mode=='all':\n",
    "        return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def unflatten_first(tensor):\n",
    "    if tensor.shape[0]==p*p:\n",
    "        return einops.rearrange(tensor, '(x y) ... -> x y ...', x=p, y=p)\n",
    "    else: \n",
    "        return tensor\n",
    "def cos(x, y):\n",
    "    return (x.dot(y))/x.norm()/y.norm()\n",
    "def mod_div(a, b):\n",
    "    return (a*pow(b, p-2, p))%p\n",
    "def normalize(tensor, axis=0):\n",
    "    return tensor/(tensor).pow(2).sum(keepdim=True, axis=axis).sqrt()\n",
    "def extract_freq_2d(tensor, freq):\n",
    "    # Takes in a pxpx... or batch x ... tensor, returns a 3x3x... tensor of the \n",
    "    # Linear and quadratic terms of frequency freq\n",
    "    tensor = unflatten_first(tensor)\n",
    "    # Extracts the linear and quadratic terms corresponding to frequency freq\n",
    "    index_1d = [0, 2*freq-1, 2*freq]\n",
    "    # Some dumb manipulation to use fancy array indexing rules\n",
    "    # Gets the rows and columns in index_1d\n",
    "    return tensor[[[i]*3 for i in index_1d], [index_1d]*3]\n",
    "def get_cov(tensor, norm=True):\n",
    "    # Calculate covariance matrix\n",
    "    if norm:\n",
    "        tensor = normalize(tensor, axis=1)\n",
    "    return tensor @ tensor.T\n",
    "def is_close(a, b):\n",
    "    return ((a-b).pow(2).sum()/(a.pow(2).sum().sqrt())/(b.pow(2).sum().sqrt())).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13280c20-1f19-451a-a82b-7b15e67f0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_components(m):\n",
    "    \"\"\"\n",
    "    Compute canonical frequency components of a real 2D signal m using fft2.\n",
    "\n",
    "    Returns:\n",
    "        components: list of dicts with keys:\n",
    "            - 'kx': frequency index in x (rows)\n",
    "            - 'ky': frequency index in y (cols)\n",
    "            - 'coeff': complex FFT coefficient at (kx, ky)\n",
    "        Nx, Ny: dimensions of the input\n",
    "    \"\"\"\n",
    "    m = np.asarray(m)\n",
    "    Nx, Ny = m.shape\n",
    "    F = np.fft.fft2(m)\n",
    "\n",
    "    components = []\n",
    "    seen = set()\n",
    "\n",
    "    for kx in range(Nx):\n",
    "        for ky in range(Ny):\n",
    "            # Conjugate partner indices for real-valued input\n",
    "            kx_conj = (-kx) % Nx\n",
    "            ky_conj = (-ky) % Ny\n",
    "\n",
    "            pair = tuple(sorted([(kx, ky), (kx_conj, ky_conj)]))\n",
    "            if pair in seen:\n",
    "                continue\n",
    "            seen.add(pair)\n",
    "\n",
    "            # Always store the \"smaller\" one in lexicographic order\n",
    "            kx_rep, ky_rep = pair[0]\n",
    "            coeff = F[kx_rep, ky_rep]\n",
    "            components.append({\n",
    "                \"kx\": kx_rep,\n",
    "                \"ky\": ky_rep,\n",
    "                \"coeff\": coeff\n",
    "            })\n",
    "\n",
    "    return components, Nx, Ny\n",
    "\n",
    "\n",
    "def components_to_terms(components, Nx, Ny, num_freqs, include_dc=True):\n",
    "    \"\"\"\n",
    "    Convert FFT components into real cosine terms with amplitude & phase.\n",
    "\n",
    "    Args:\n",
    "        components: output from compute_frequency_components\n",
    "        Nx, Ny: grid size\n",
    "        num_freqs: number of *non-DC* frequencies to keep\n",
    "        include_dc: whether to include the constant (DC) term\n",
    "\n",
    "    Returns:\n",
    "        terms: list of dicts with keys:\n",
    "            - 'kx', 'ky'\n",
    "            - 'amplitude'  (real, >= 0)\n",
    "            - 'phase'      (in radians)\n",
    "            - 'is_dc'      (bool, True only for the constant term)\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    dc_term = None\n",
    "\n",
    "    for c in components:\n",
    "        kx = c[\"kx\"]\n",
    "        ky = c[\"ky\"]\n",
    "        coeff = c[\"coeff\"]\n",
    "\n",
    "        # DC component (constant offset)\n",
    "        if kx == 0 and ky == 0:\n",
    "            dc_value = coeff.real / (Nx * Ny)\n",
    "            dc_term = {\n",
    "                \"kx\": 0,\n",
    "                \"ky\": 0,\n",
    "                \"amplitude\": dc_value,  # directly the constant offset\n",
    "                \"phase\": 0.0,\n",
    "                \"is_dc\": True,\n",
    "            }\n",
    "        else:\n",
    "            # For real signals, combine (k, l) and (-k, -l) into:\n",
    "            #   2*|F|/(Nx*Ny) * cos(2π(kx i/Nx + ky j/Ny) + phase)\n",
    "            amp = 2.0 * np.abs(coeff) / (Nx * Ny)\n",
    "            phase = np.angle(coeff)\n",
    "            terms.append({\n",
    "                \"kx\": kx,\n",
    "                \"ky\": ky,\n",
    "                \"amplitude\": amp,\n",
    "                \"phase\": phase,\n",
    "                \"is_dc\": False,\n",
    "            })\n",
    "\n",
    "    # Sort non-DC terms by amplitude (largest first)\n",
    "    terms.sort(key=lambda t: t[\"amplitude\"], reverse=True)\n",
    "\n",
    "    # Keep only the top num_freqs\n",
    "    terms = terms[:num_freqs]\n",
    "\n",
    "    # Optionally prepend DC term\n",
    "    if include_dc and dc_term is not None:\n",
    "        terms = [dc_term] + terms\n",
    "\n",
    "    return terms\n",
    "\n",
    "def generate_python_reconstructor(terms, Nx, Ny, func_name=\"recon_func\", decimals=4):\n",
    "    \"\"\"\n",
    "    Generate a Python function as a string that reconstructs the signal\n",
    "    using explicit 1D and product-of-cosine terms from a sparse Fourier representation.\n",
    "\n",
    "    Types of terms:\n",
    "      - DC term (constant)\n",
    "      - Row-only:   A * cos(2π * (kx*i/Nx) + φ)\n",
    "      - Col-only:   B * cos(2π * (ky*j/Ny) + φ)\n",
    "      - Product:    C * cos(2π * (kx*i/Nx) + φ) * cos(2π * (ky*j/Ny) + φ)\n",
    "\n",
    "    You must have `import numpy as np` in the scope where you paste/run this.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(i, j):\")\n",
    "\n",
    "    if not terms:\n",
    "        lines.append(\"    return 0.0\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Separate DC from others\n",
    "    dc_terms = [t for t in terms if t.get(\"is_dc\", False)]\n",
    "    non_dc_terms = [t for t in terms if not t.get(\"is_dc\", False)]\n",
    "\n",
    "    if dc_terms:\n",
    "        dc_val = round(dc_terms[0][\"amplitude\"], decimals)\n",
    "        lines.append(f\"    val = {dc_val}\")\n",
    "    else:\n",
    "        lines.append(\"    val = 0.0\")\n",
    "\n",
    "    for t in non_dc_terms:\n",
    "        kx, ky = t[\"kx\"], t[\"ky\"]\n",
    "        A = round(t[\"amplitude\"], decimals)\n",
    "        phi = round(t[\"phase\"], decimals)\n",
    "\n",
    "        if A == 0:\n",
    "            continue\n",
    "\n",
    "        if kx != 0 and ky == 0:\n",
    "            # Row-only cosine\n",
    "            line = (\n",
    "                f\"    val += {A} * np.cos(2*np.pi*(({kx}*i)/{Nx}) + {phi})\"\n",
    "            )\n",
    "        elif kx == 0 and ky != 0:\n",
    "            # Column-only cosine\n",
    "            line = (\n",
    "                f\"    val += {A} * np.cos(2*np.pi*(({ky}*j)/{Ny}) + {phi})\"\n",
    "            )\n",
    "        else:\n",
    "            # Product-of-cosines term (your desired \"third\" type)\n",
    "            line = (\n",
    "                f\"    val += {A} * np.cos(2*np.pi*(({kx}*i)/{Nx}) + {phi})\"\n",
    "                f\" * np.cos(2*np.pi*(({ky}*j)/{Ny}) + {phi})\"\n",
    "            )\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "    lines.append(\"    return val\")\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b31bb-66b2-4ac4-ad40-1c565cff3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For all CUDA devices\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False # Set to False for determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c9596-9056-49dc-9125-f6d78378faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_frequency_penalty(model: Transformer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encourage neighboring tokens in the embedding space to be similar.\n",
    "    This is a graph-Laplacian-style smoothness penalty on W_E, which\n",
    "    corresponds to penalizing high-frequency Fourier modes on Z_p.\n",
    "\n",
    "    W_E has shape [d_model, d_vocab].\n",
    "    We smooth along the vocab dimension (the modular arithmetic axis).\n",
    "    \"\"\"\n",
    "    E = model.embed.W_E  # [d_model, d_vocab]\n",
    "\n",
    "    # Differences between consecutive tokens (0-1, 1-2, ..., (V-2)-(V-1))\n",
    "    diff = E[:, 1:] - E[:, :-1]  # [d_model, d_vocab-1]\n",
    "\n",
    "    # Wrap-around difference to respect modular structure: token V-1 vs 0\n",
    "    wrap = E[:, :1] - E[:, -1:]  # [d_model, 1]\n",
    "\n",
    "    # Mean squared differences (Laplacian smoothness)\n",
    "    penalty = (diff.pow(2).mean() + wrap.pow(2).mean())\n",
    "    return penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e3acf-9d18-48b9-9bf6-0cd795858268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f80eb1a8-a566-404a-a766-19a19d918205",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686f161-f5b6-46b6-baaf-2761c3064ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/home/stephen/grokking')\n",
    "\n",
    "\n",
    "p=113\n",
    "# seed=2 ##Set at top\n",
    "\n",
    "lr=1e-3 #@param\n",
    "p=113 #@param\n",
    "d_model = 128 #@param\n",
    "fn_name = 'add' #@param ['add', 'subtract', 'x2xyy2','rand']\n",
    "num_epochs = 20000 #@param\n",
    "save_models = True #@param\n",
    "save_every = 10 # Maybe crank this down\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1 #@param\n",
    "\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1\n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "# batch_size = 512\n",
    "use_ln = False\n",
    "random_answers = np.random.randint(low=0, high=p, size=(p, p))\n",
    "fns_dict = {'add': lambda x,y:(x+y)%p, 'subtract': lambda x,y:(x-y)%p, 'x2xyy2':lambda x,y:(x**2+x*y+y**2)%p, 'rand':lambda x,y:random_answers[x][y]}\n",
    "fn = fns_dict[fn_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5953f-f4cb-459b-b2a4-55a8163135d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_train, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8335ca5-3a4a-4e8c-a543-91efc3b8a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_test(frac_train, num, seed=0):\n",
    "    # Generate train and test split\n",
    "    pairs = [(i, j, num) for i in range(num) for j in range(num)]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "    div = int(frac_train*len(pairs))\n",
    "    return pairs[:div], pairs[div:]\n",
    "\n",
    "train, test = gen_train_test(frac_train, p, seed)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1c010-e637-4e6a-9acc-b5ec2720fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an array of Boolean indices according to whether each data point is in \n",
    "# train or test\n",
    "# Used to index into the big batch of all possible data\n",
    "is_train = []\n",
    "is_test = []\n",
    "for x in range(p):\n",
    "    for y in range(p):\n",
    "        if (x, y, 113) in train:\n",
    "            is_train.append(True)\n",
    "            is_test.append(False)\n",
    "        else:\n",
    "            is_train.append(False)\n",
    "            is_test.append(True)\n",
    "is_train = np.array(is_train)\n",
    "is_test = np.array(is_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813db3e0-45d8-48aa-bb8c-f8920a52d64f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb273b-eabd-432b-ba2a-72f3e7ac04f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "model = Transformer(num_layers=num_layers, d_vocab=d_vocab, d_model=d_model, d_mlp=d_mlp, d_head=d_head, num_heads=num_heads, n_ctx=n_ctx, act_type=act_type, use_cache=False, use_ln=use_ln)\n",
    "model.to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "run_name = f\"grok_{int(time.time())}\"\n",
    "print(f'Run name {run_name}')\n",
    "if save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "    save_dict = {'model':model.state_dict(), 'train_data':train, 'test_data':test}\n",
    "    torch.save(save_dict, root/run_name/'init.pth')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 1. Compute base cross-entropy losses (no regularization)\n",
    "    train_ce_loss = full_loss(model, train)\n",
    "    test_loss = full_loss(model, test)  # leave test as-is\n",
    "\n",
    "    # 2. Compute smoothness / frequency penalty on embeddings\n",
    "    freq_pen = embedding_frequency_penalty(model)\n",
    "\n",
    "    # 3. Total train loss = CE + λ * penalty\n",
    "    train_loss = train_ce_loss + freq_penalty_weight * freq_pen\n",
    "\n",
    "    # 4. Track (usually you want to log CE only for grokking plots)\n",
    "    train_losses.append(train_ce_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    logits = model(train)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in train]).to('cuda')\n",
    "    train_acc=((logits.detach().cpu().argmax(1)==labels.detach().cpu()).sum()/len(labels)).item()\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    logits = model(test)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in test]).to('cuda')\n",
    "    test_acc=((logits.detach().cpu().argmax(1)==labels.detach().cpu()).sum()/len(labels)).item()\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f\"{epoch}_\"\n",
    "            f\"{np.log(train_ce_loss.item()):.4f}_\"\n",
    "            f\"{np.log(test_loss.item()):.4f}_\"\n",
    "            f\"pen={freq_pen.item():.3e}\"\n",
    "        )\n",
    "\n",
    "    # 5. Backprop / step\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    if test_loss.item() < stopping_thresh:\n",
    "        break\n",
    "    if (save_models) and (epoch%save_every == 0):\n",
    "        if test_loss.item() < stopping_thresh:\n",
    "            break\n",
    "        save_dict = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_dict, root/run_name/f\"{epoch}.pth\")\n",
    "        print(f\"Saved model to {root/run_name/f'{epoch}.pth'}\")\n",
    "if not save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "save_dict = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_loss': train_loss,\n",
    "    'test_loss': test_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'test_accs':test_accs,\n",
    "    'epoch': epoch,\n",
    "}\n",
    "torch.save(save_dict, root/run_name/f\"final.pth\")\n",
    "print(f\"Saved model to {root/run_name/f'final.pth'}\")\n",
    "\n",
    "# save_models = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2baa961-29ae-48ec-b2b0-8a0cc3693fe9",
   "metadata": {},
   "source": [
    "## Viz Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e881088-67fa-44cb-8b18-6a58c9dd4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHILL_BROWN='#948979'\n",
    "YELLOW='#ffd35a'\n",
    "YELLOW_FADE='#7f6a2d'\n",
    "BLUE='#65c8d0'\n",
    "GREEN='#6e9671' \n",
    "CHILL_GREEN='#6c946f'\n",
    "CHILL_BLUE='#3d5c6f'\n",
    "FRESH_TAN='#dfd0b9'\n",
    "CYAN='#00FFFF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca07b3-d8c5-45e9-9bc0-57f5abbb6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accs)\n",
    "plt.plot(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed9ef2-c7cd-4733-8bb5-c4dfe3d92ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()        \n",
    "fig=plt.figure(0,(14,6), facecolor='k') \n",
    "ax=fig.add_subplot(111, facecolor='k')\n",
    "\n",
    "plt.plot(train_accs[:500], c=BLUE, linewidth=2.5, alpha=0.95) #'#dfd0b9')\n",
    "plt.plot(test_accs[:500], c=YELLOW, linewidth=2.5, alpha=0.95) #'#dfd0b9')\n",
    "plt.tick_params(axis='x', colors='#948979', labelsize=16)  # Add labelsize here\n",
    "plt.tick_params(axis='y', colors='#948979', labelsize=16)  # Add labelsize here\n",
    "\n",
    "plt.gca().spines['top'].set_color('#948979')\n",
    "plt.gca().spines['bottom'].set_color('#948979')\n",
    "plt.gca().spines['left'].set_color('#948979')\n",
    "plt.gca().spines['right'].set_color('#948979')\n",
    "plt.subplots_adjust(bottom=0.2)  # Add fixed bottom margin\n",
    "# plt.xlim([-10, len(to_plot)+10])\n",
    "plt.ylim([0,1.05]) #Bad idea to fix or is it fine?\n",
    "plt.grid(True, color='#948979', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08b966-4b81-48dc-ab1e-b8e7d483c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()        \n",
    "fig=plt.figure(0,(14,6), facecolor='k') \n",
    "ax=fig.add_subplot(111, facecolor='k')\n",
    "\n",
    "plt.plot(train_accs, c=BLUE, linewidth=2.5, alpha=0.95) #'#dfd0b9')\n",
    "plt.plot(test_accs, c=YELLOW, linewidth=2.5, alpha=0.95) #'#dfd0b9')\n",
    "plt.tick_params(axis='x', colors='#948979', labelsize=16)  # Add labelsize here\n",
    "plt.tick_params(axis='y', colors='#948979', labelsize=16)  # Add labelsize here\n",
    "\n",
    "plt.gca().spines['top'].set_color('#948979')\n",
    "plt.gca().spines['bottom'].set_color('#948979')\n",
    "plt.gca().spines['left'].set_color('#948979')\n",
    "plt.gca().spines['right'].set_color('#948979')\n",
    "plt.subplots_adjust(bottom=0.2)  # Add fixed bottom margin\n",
    "# plt.xlim([-10, len(to_plot)+10])\n",
    "plt.ylim([0,1.05]) #Bad idea to fix or is it fine?\n",
    "plt.grid(True, color='#948979', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4c94a-4b3d-4408-a4dd-280b0a9b327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853aae2-82a1-4651-8585-010c8429d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.tensor([(i, j, p) for i in range(p) for j in range(p)]).to('cuda')\n",
    "labels = torch.tensor([fn(i, j) for i, j, _ in all_data]).to('cuda')\n",
    "cache = {}\n",
    "model.remove_all_hooks()\n",
    "model.cache_all(cache)\n",
    "# Final position only\n",
    "original_logits = model(all_data)[:, -1]\n",
    "# Remove equals sign from output logits\n",
    "original_logits = original_logits[:, :-1]\n",
    "original_loss = cross_entropy_high_precision(original_logits, labels)\n",
    "print(f\"Original loss: {original_loss.item()}\")\n",
    "\n",
    "#Ok let's try pre -> the og. \n",
    "mr = einops.rearrange(cache['blocks.0.mlp.hook_pre'], \"(x y) ... -> x y ...\", x=p).detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7c86a-3833-4e9e-a4cc-bfec1f42a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper for classifying term type ----\n",
    "def classify_term_type(kx, ky, is_dc):\n",
    "    if is_dc:\n",
    "        return \"dc\"\n",
    "    if kx != 0 and ky == 0:\n",
    "        return \"row\"\n",
    "    if kx == 0 and ky != 0:\n",
    "        return \"col\"\n",
    "    if kx == 0 and ky == 0:\n",
    "        return \"dc\"\n",
    "    return \"cross\"  # kx != 0 and ky != 0\n",
    "\n",
    "\n",
    "# ---- Loop over neurons and build DataFrame ----\n",
    "all_rows = []\n",
    "\n",
    "for neuron_idx in tqdm(range(512)):\n",
    "    # 113 x 113 slice for this neuron\n",
    "    m_slice = mr[:, :, 2, neuron_idx]\n",
    "\n",
    "    components, Nx, Ny = compute_frequency_components(m_slice)\n",
    "    terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "\n",
    "    # Max amplitude (non-DC) for this neuron, for \"strong\" comparison later\n",
    "    non_dc_terms = [t for t in terms if not t.get(\"is_dc\", False)]\n",
    "    max_amp = max((t[\"amplitude\"] for t in non_dc_terms), default=0.0)\n",
    "\n",
    "    for rank, t in enumerate(terms):\n",
    "        is_dc = t.get(\"is_dc\", False)\n",
    "        kx = t[\"kx\"]\n",
    "        ky = t[\"ky\"]\n",
    "        amp = t[\"amplitude\"]\n",
    "        phase = t[\"phase\"]\n",
    "\n",
    "        term_type = classify_term_type(kx, ky, is_dc)\n",
    "\n",
    "        all_rows.append({\n",
    "            \"neuron_idx\": neuron_idx,\n",
    "            \"term_rank\": rank,          # 0 = DC if include_dc=True\n",
    "            \"is_dc\": is_dc,\n",
    "            \"term_type\": term_type,     # \"dc\", \"row\", \"col\", \"cross\"\n",
    "            \"kx\": kx,\n",
    "            \"ky\": ky,\n",
    "            \"amplitude\": amp,\n",
    "            \"phase\": phase,\n",
    "            \"max_amp_neuron\": max_amp,  # per neuron\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df['relative_amplitude']=df['amplitude']/df['max_amp_neuron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5188a-79f1-46d6-9e72-c02e5415c2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['kx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f84aee-19d0-46a4-ba20-1de9caf7c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df[df['kx']==5]\n",
    "dff=dff[dff['ky']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f68b17-2262-4cf0-9dca-2190502932eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.sort_values(by='relative_amplitude', ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c6322-32b5-4f99-ac50-2c7761dfef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_idx=443\n",
    "FT = np.fft.fft2(mr[:,:,2,neuron_idx])\n",
    "\n",
    "components, Nx, Ny = compute_frequency_components(mr[:,:,2,neuron_idx])\n",
    "terms = components_to_terms(components, Nx, Ny, num_freqs=4, include_dc=True)\n",
    "code_str = generate_python_reconstructor(terms, Nx, Ny,\n",
    "                                         func_name=\"approx_M\",\n",
    "                                         decimals=3)\n",
    "print(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5f5a3-a0bc-4394-bdd6-cf2ba8eb5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_M(i, j):\n",
    "    val = 0.179\n",
    "    val += 0.044 * np.cos(2*np.pi*((5*i)/113) + 0.408) * np.cos(2*np.pi*((5*j)/113) + 0.408)\n",
    "    val += 0.043 * np.cos(2*np.pi*((5*i)/113) + 0.565)\n",
    "    val += 0.043 * np.cos(2*np.pi*((5*j)/113) + 0.561)\n",
    "    val += 0.035 * np.cos(2*np.pi*((3*i)/113) + 2.824) * np.cos(2*np.pi*((3*j)/113) + 2.824)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70b0c9-98e0-4547-9cc8-0409363fd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_approx=np.zeros((p,p))\n",
    "for i in range(p):\n",
    "    for j in range(p):\n",
    "        m_approx[i,j]=approx_M(i,j)\n",
    "\n",
    "fig=plt.figure(0, (12, 6))\n",
    "fig.add_subplot(1,3,1); plt.imshow(mr[:,:,2,neuron_idx]); plt.axis('off')\n",
    "fig.add_subplot(1,3,2); plt.imshow(np.abs(FT));  plt.axis('off')\n",
    "fig.add_subplot(1,3,3); plt.imshow(m_approx); plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ca9c3-d7c0-474b-9dd9-e9ae2871c10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30eecdc-08c5-4fe8-888a-35a86f1050d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863c5c2-61ea-44cb-aafb-64a4d34d6ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
